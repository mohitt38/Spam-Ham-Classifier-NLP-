# üìß Spam vs Ham Classifier

This project implements a **Spam-Ham (SMS Spam Detection)** classifier using **Natural Language Processing (NLP)** and **Machine Learning**.  
It demonstrates text preprocessing, feature extraction using **Bag of Words (BoW)** and **TF-IDF**, and classification using **Naive Bayes** (`MultinomialNB`).

---

## üìÇ Project Overview

The goal is to classify SMS messages as:
- **Spam**: Unwanted or unsolicited messages.
- **Ham**: Legitimate, non-spam messages.

---

## üõ†Ô∏è Technologies Used

- **Python 3.x**
- **NLTK** ‚Äì Tokenization, stopwords removal
- **scikit-learn** ‚Äì Feature extraction (BoW, TF-IDF) and ML models
- **Pandas / NumPy** ‚Äì Data handling
- **Matplotlib / Seaborn** ‚Äì Visualization

---

## üîÑ Workflow

### 1Ô∏è‚É£ Data Preprocessing
- Lowercasing text
- Removing punctuation and numbers
- Tokenization
- Stopwords removal
- Lemmatization

### 2Ô∏è‚É£ Feature Extraction
- **Bag of Words (BoW)**: Counts of words in messages.
- **TF-IDF**: Weighted representation of word importance.

### 3Ô∏è‚É£ Model Training
- **Algorithm:** Multinomial Naive Bayes (`MultinomialNB`)
- **Why Naive Bayes?**
  - Works well with text classification
  - Efficient for large datasets
  - Performs exceptionally well with word frequency-based features

### 4Ô∏è‚É£ Model Evaluation
- Metrics: Accuracy, Precision, Recall, F1-score
- Achieved **98% Accuracy** using TF-IDF + MultinomialNB

---

## üìà Results

| Feature Extraction | Algorithm       | Accuracy |
|--------------------|----------------|----------|
| Bag of Words (BoW) | MultinomialNB  | 96%      |
| TF-IDF             | MultinomialNB  | **98%**  |

---

## üìå Key Takeaways
- TF-IDF generally performs better than BoW for spam detection.
- Naive Bayes is a strong baseline for text classification problems.
- Proper preprocessing (stopwords removal, lemmatization) boosts performance.

---


